a todos, principalmente con SQL y con Haciendo Reportería después quedé con Inteligencia de Negocio un par de años al segundo punto, ellos se salieron un poco más a distrata y me vieron muy similar, sí, trabajando en SQL y de ahí hasta un ingeniero de datos así que sí, es una parte que más me gusta de la parte de Ingeniería más que de parte de software y desarrollo Vale, vale, bueno Respecto a la experiencia de GCP ¿Cuáles son los servicios principales de GCP que ha utilizado en los proyectos? ¿Y cómo integró en algún escenario de negocio particular? A ver, lo principal que originalmente tenía disponible han sido Composer, Storage, BigQuery por ahí, claro, Dataflow pero se lo he usado un poco menos a veces Secret y esas cosas como más pero principalmente, claro Composer, Flow, Storage, BigQuery principalmente, cuando me toca trabajar con datos ya sea estructurado o no y pieza de datos principalmente me toca trabajar con BigQuery como van a decir de SQL y eso y de ahí, por ejemplo, me ha tocado por ejemplo, armar un DAT tomar los datos desde una base y ponerlos en un database como más refinado acorde a la necesidad entonces, hacer un Dataflow que en el fondo pueda tomar datos, ya sea desde Flow, Storage o desde el BigQuery analizarlo, transformarlo y finalmente insertarlo también en el mismo BigQuery eso ha sido como el flujo principal me ha tocado trabajar también a veces con, siempre un poco en la base de tomar los datos desde un Storage o un BigQuery, con Python me ha tocado trabajar un poco trabajando con Jupyter Notebook en Vertex AI pero principalmente ha sido eso, ha sido un trabajo en BigQuery como el análisis en sí y con Storage los otros servicios son más, claro los acompañamientos, pero lo principal ha sido como el conjunto, unir los datos desde un lado o dejarlos, o tomarlos de una API dejarlos en el peragüez eso ha sido lo principal Vale, respecto a a su experiencia en Python ¿Qué librería o framework de Python utiliza para los análisis de datos y cómo optimiza el código en este lenguaje? A ver, mi experiencia con Python claro, no es tanta tampoco a él le gustó el método de utilizarlo pero principalmente me ha tocado trabajar con casi 100 librerías a lo que me ha gustado trabajar lo más lo que hago es como diccionario como lista, generalmente con las transformaciones de JSON o ABRO entonces generalmente muchas librerías como por ejemplo Pandas no me las voy a utilizar tanto, así más que nada las librerías como para generar estos archivos ABRO eso, pero así como alguna librería en particular como de análisis como Pandas, claro no me ha tocado yo más trabajo como de diccionario de datos Vale Con respecto a la programación de estos objetos, bueno en vista que tenía igual poca experiencia en programación pero igual, ¿me puedes explicar los principios de programación de estos objetos? La verdad ese concepto no lo tengo claro ahora Ya, vale Sobre SQL y tu experiencia con SQL, ¿cómo optimizas una consulta que tarda demasiado tiempo en ejecutarse? ¿Cuáles son los pasos que tú usas para ir mejorando esa consulta? Bueno principalmente, creo que va a depender mucho de lo que necesites, pero claro obviamente lo más básico es elegir los campos que simplemente necesitas y no toda la base lo otro también es hacer los fitos, tenerlos súper claros, a veces si tengo que consultar ahora por ejemplo una base de la TAM claro, es decir, lo importante es consultar por el país que necesito no hacer esa, principalmente esos son los fitos, luego de eso tratar de no ordenar, porque solamente voy a consumar estos recursos y bueno eso principalmente dependiendo de lo que se necesita vale mayormente Respecto al modelamiento, integración y proceso de datos ¿Puedes describir algún proyecto donde diseñaste un modelo de datos eficiente y ¿cómo lo integraste con los distintos orígenes? Y bueno, a ver Tuve uno que fue como más grande tiempo atrás yo lo trabajé como inteligencia de negocio básicamente yo propuse hacer un Data Warehouse en ese entonces era con S-Ware operacional en ese momento y bueno, básicamente yo tenía una reportería montada con ClipView pero en Excel o con alguna exportación que había, etc La cosa es que propuse hacer un Data Warehouse para tomar los distintos datos esto era en un instituto académico profesional, entonces había dos datos, por ejemplo, que venían del curso humano que venían de ese entonces, una base de datos de payroll que venían los datos de académicos que los generaba el mismo tema de TI que habían datos financieros que a veces venían solo de Excel o un CCD, entonces toda esa información que estaba tirada por todos lados que necesitaba ser mostrada al final la tomé la hizo el distinto ETL dependiendo de la fuente para generar y descentralizar todos los datos de un Data Warehouse y después el Data Warehouse era el que se alimentaba por la reportería ClipView básicamente eso es lo que, por ejemplo, un trabajo más, el más grande que he hecho en realidad respecto a como armar todo el proceso en otros lados yo he llegado con las cosas reales donde me ha tocado mantener o agregar data más en función de lo que ya estaba hecho obviamente no cambiar nada o también lo que se dice en una de las experiencias fue tomar data de distintos API, o sea, de distintas softwares que usaba la empresa y que las datas de ellos estaban expuestas por API, entonces ir a conectarse a la API por Python y esos datos también transformarlos en un JSON, un ABL para finalmente cargar en el BigQuery en ese caso Vale, y en el caso de BigQuery, ¿cómo manejas la optimización de costo al ejecutar las consultas? ¿te ha tocado optimizar a ese nivel? Sí, o sea, bueno es que principalmente, claro, a ver mmm lo principal, por ejemplo, ¿cómo decirlo? es que la optimización tiene que ver ahí con el tratar de hacer las queries lo más específicas posibles, por ejemplo un dato, veo el preview, me voy a poner a hacer una query para hacer eso y claro, y ahí tener bien claro, como digo yo, los fichas que se necesitan en base a la problemática que hay ahí en tema específico, o sea, eso menor va a ser el costo de de la query en sí Vale En la experiencia del desarrollo de ETLs ¿tienes alguna estrategia para diseñar y monitorear los ETLs de manera robusta? Estrategia como tal mmm No, estrategia como tal, últimamente no me ha tocado más que nada trabajar con análisis generalmente cuando hacía lo de ETL eh, oye, me acuerdo que en SQL habían cosas de monitoreo pero no recuerdo también, y ahora hoy en día, no, no me ha tocado mucho eso la verdad Vale, pero te ha tocado el desarrollo de ETL, ¿no? ¿y quién lo ha hecho? Bueno, en su momento me tocó con con Microsoft Services, cuando trabajaba más que nada con Microsoft, y en un último tiempo ya trabajando con con BigQuery o con GCP, me tocó trabajar un poco con Beam, aunque se lo pido un poco y lo otro lo trabajaba más que nada como con como decía yo, como tomar los datos desde la distinta API, que eso fue lo que más me tocó en una experiencia, tomar los datos desde la distinta API, ellos los querían por algún motivo en archivos JSON que fueran a bucket, o sea datos que vienen de AARP en el bucket, tanto para todo, etc. Entonces generalmente me soldaba esos DAGs, un Python que iba, bueno, que básicamente tomaba esta API, tomaba los datos los pasaba este JSON, los dejaba guardados, y ahí las palabras estaban montadas en estos, hacia estos buckets las palabras de BigQuery ¿Esos DAGs son de Apache Airflow? Sí, de Apache Airflow ¿Y cómo desarrollaste esos DAGs? ¿Para que fueran reutilizables por otros procesos? Bueno, había bueno, de hecho había varias estaban separados por decirlo de alguna manera había una función que por ejemplo cargaba los datos al bucket, entonces obviamente el parámetro era el bucket de destino, por ejemplo y eso cambiaba dependiendo de la API que estaba consumiendo entonces se separaba en base a las funciones o había otras funciones que generalmente lo generaba el JSON entonces había que borrarlo del JSONL, entonces había otra función que hacía esa parte había otra función que iba a buscar el secret, por ejemplo dependiendo de lo que se necesitara bueno, la servicia con lo usaba dependiendo obviamente del ambiente y ya estaba como separado en distintas funciones que hacían todo este proceso, bueno aquí te digo como que se pusiera más reutilizable el en general ¿Uso de JIT me imagino que sí? ¿Cierto? ¿En caso de que tengáis algún conflicto de fuentes, como lo lo resolvías con JIT? Por ejemplo, dos personas tocaron el mismo fuente y trataron de subir al mismo tiempo el código y cuando hace el pull o sea el push, perdón te dice, hay un conflicto y hay que resolver el conflicto ¿O no te ha tocado? Sí, no, no, sí, es que asumiendo que son, es que estoy asumiendo que los dos serían correctos, sean necesarios a ver, la forma más quizás simple que lo suelo hacer es con un merge de la rama quizás, bueno, a veces toca en general como una rama copia para después hacer el merge, por ejemplo una copia de la rama A para después hacer el merge de esa rama, de la copia B y después al final puchar esa rama y las otras dejarlas, bueno dejarlas ahí no, o incorporarlas pero un punto, tratando de echar esas dos asumiendo que obviamente los dos hits son, o los dos pushes son correctos vale vale ¿Conoces Dataflow? Sí, me ha tocado me toca más que hoy en día verlo, analizarlo pero no me ha tocado generar tanto tanto Dataflow en sí ¿Pero te ha tocado hacer un pipeline, por ejemplo? Sí, como digo, una vez me tocó agregar como otra otra pata que había de un data de un datamark, de un data warehouse que había que era con Dataflow entonces me tocó usarlo con con Dataflow, a mi me hubiera dicho de hecho era el macro de Dataflow que generaba todo este proceso, pero claro lo trabajé muy poco, entonces no lo tengo de fundación, lo he trabajado constantemente y pues... ¿Vas a trabajar con Terraform para la creación de infraestructura en GCP? No, no me voy a trabajar con Terraform Vale, ¿te ha tocado usar Jenkins como herramienta de integración continua? Sí, pero me toca más que nada usarlo yo de algo que ya está montado o sea, está el pipeline para generar el release de lo que necesito no es algo que yo haya tenido que configurar hacer yo el Jenkins, no, por eso sí me ha tocado moverlo, ver el monitoreo pero no generarlo yo, no programar yo ese Jenkins Vale Respecto a matemáticas ¿Qué aplicaciones matemáticas has utilizado en algún análisis de datos? No me ha tocado utilizar aplicaciones matemáticas como tal Respecto a tu conocimiento de integración continua o continuous delivery ¿Qué cosas has utilizado en estos procesos de integración continua para mejorar la calidad del desarrollo? Básicamente esas partes las ven más las arquitecturas, como que la parte de 6C a mí no me toca mucho, es como más que nada mandar pull, mandar push y listo ¿Pero no te exigen, por ejemplo, prueba unitaria o algún estándar de calidad como para decir, ah, mi código funciona? A ver, en la práctica estética hay sistemas de prueba más seguido y bueno, hoy en día al menos lo que me ha tocado en la experiencia es que respecto al 6C es en lo que me argumento la burocracia financiera siguen siendo pruebas como nivel de de probar el código así como en el ambiente previo como eso, además el ambiente previo funciona en test, por ejemplo, para hacer un ambiente preproductivo pero claro, no es como tan continuo como suena, como debería ser en la teórica del disco 6 Vale Fabián Yo ya terminé con las preguntas ahora si tú me puedes contar algún proyecto que te haya entusiasmado técnicamente algún concepto que no hemos hablado y que a ti te interese contarme Bueno, como decía originalmente el tema me ha dado mucho gusto entonces cuando comencé ese proyecto fue lo que más me gustó me... me hace me hizo más sentido en el sentido de que me gustaba de que sabía por donde iba entonces como el tema de los datos, de trabajarlo de poder integrar esa información que en el fondo en muchos lados la información está tirada por todos lados en base a eso, por ejemplo, lo de IMA a mi me gustó harto obviamente lo escribí en su momento para generarlo pero claro, por temas de proyectos teorías, etc solo me tocó verlo un poco entonces eso por ejemplo me mostraría como me ha ayudado un poco más el tema de aprender bien con BIM, de hacer los datos en general como que eso a veces he estado en el tiempo limitado por funciones, no me ha tocado como tal meterme en esos temas de BIM o de generar los datos en bloco, entonces el tema también de todo tipo de los datos streaming, etc obviamente me ha tocado trabajar con datos batch streaming en algún que otro curso que he hecho, pero la realidad me ha tocado solo Batch Vale, vale Ya pues, yo estoy listo ya con mis preguntas lo que va a pasar ahora es que yo tengo que hacer un feedback de esto y entregárselo a Matías, y Matías ahí de acuerdo a los resultados del de la calculadora que tenemos avanzamos con el proceso, dale así que Fabiola un gusto y que te vaya súper bien y éxito en tu cultura central Obviamente, gracias Gracias, nos vemos, chao chao