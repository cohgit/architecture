¿Cómo se llama? Preciso nomás, para que podamos hacer más preguntas, y después al final, si quieres, profundizamos en algún tema que te haya quedado como en la duda, o que querías reforzar algo, ¿ya? Ok. Experiencia de XCP, estimado, ¿cuáles son los servicios principales de XCP que has utilizado en tu proyecto? Principalmente Cloud Storage, Composer, Dataflow, Hardstack, y... ¿Qué es experiencia? Bien, eso principalmente, pero también está Cloud Mode, Cloud Monitoring, Cloud Functions, y... ¿Qué más? Eso principalmente, la sección de ahí, de allá... Perfecto. Oye, experiencia en Python, ¿qué librería o framework de Python utilizas generalmente para el análisis de datos? Para análisis de datos, en su momento era Pandas o Polar ahora, pero principalmente eso. Vale. Sobre experiencia orientada a programación orientada a objetos, ¿me podés contar algún principio de programación orientada a objetos? ¿Principio de programación orientada a objetos? No sé, habría que responder. ¿Pero qué te acuerdas de orientada a objetos? Teóricamente, en este momento... Sí, es una pregunta muy histórica. Sobre SQL, cuando tienes una consulta que tarda demasiado tiempo, ¿cuáles son los pasos que tú aplicas para optimizarla? ¿Cuáles son los pasos? O sea, dependiendo del sistema, se puede poner un sistema de optimizadores para ver en qué parte de la consulta se está decorando o construyendo más. Aquí directamente en MySQL, y puedes verlo ahí en la consulta, en qué parte se decoraba. Pero principalmente, un fallo de analizar, ver bien los drawings que se están haciendo, fijarme bien en los filtros que se estén quitando, preocuparme también de no traer todas las columnas, sino las que estamos necesitando, y eso principalmente. Trabajo con los drawings, la web, y que esté trayendo las columnas que realmente van a necesitar y no las necesito. Cuando toca diseñar un modelo de datos, ¿cómo lo hace el diseño y con qué orígenes de datos ha tocado alimentar ese modelo? Actualmente, estamos trabajando con, nosotros nos encargamos de llevar la data de SAP a GCP, y también de algunas fuertes órdenes, y algunos datos. Todo esto a través de datos inteligentes, o sea, los datos de GCP, todos estos archivos como parquet, y ahí tenemos un filename, que son diversos tags por tabla, pero si bien tenemos un tag template, en base a un JSON con parámetros de cada tabla, genera los tags de cada tabla, como por aquí, y todos siguen la misma lógica de procesamiento. Reconocen si en el parquet hay datos, datos nuevos, en el cambio de las tablas de SAP, si hay datos, hay un proceso con data flow de limpieza, y después de esa limpieza pasa a crear una tabla temporal en BigQuery, donde se cargan estos nuevos archivos, después se inercia la tabla en así de sistema original, y ese es el proceso, y aquí tenemos, como digo, replicamos las tablas tal como están en el origen, y después de eso, a otros proyectos de GCP se consumen estas tablas, y se va generando lo que nosotros llamamos los PRF, pero ahí está en cada modelo, y ya se da cuenta de todas las tablas que se pueden cobrar. Sí, es la técnica para no pagar la licencia digital de SAP. Eso es bien, eso es bien. Si no te llega auditoría de SAP y te van a dar el corte. Muy bien. Bueno, creo que ya me contaste sobre la optimización de BigQuery, porque la pregunta de recién era como para base de datos, pero respondiste las dos, me parece. Sí, pero hoy, para comentarte, aquí en BigQuery siempre estamos analizando el tema del billing. Eso, ¿cómo revisan el tema, el costo con BigQuery? O sea, tenemos la típica cuenta de presupuesto, que nos va avisando cuándo nos vamos bajando del presupuesto, o cuándo estamos llegando con el billar. Pero aquí igual, son pocos los que metemos, como en esta tabla, que se le llama un report, son pocos los que metemos en las manos. Todos estamos alineados de ver cuánto consumen las consultas, no tenemos limitaciones por el billar, estamos verificando que la consulta no se vaya a $100 una consulta, pero ahí estamos tratando siempre de particionar, clasificar de la mejor forma las tablas para que las consultas sean las más optimizadas. Perfecto. Sobre los ETL, ¿qué herramientas ocupan y cómo hacen que los ETL sean robustos? ¿Robustos? ¿En qué sentido robustos? No sé, podríamos decir cómo los robotizan, cómo los parametrizan. Mira, como te comentaba, por así decirse, la herramienta de ETL es más que ETL, es más de migración, de transporte de datos. Con eso nos ayudamos los datos tal como se ejecutan los cambios de tabla hacia la nube. Y ahí tenemos, como te dije, el pipeline, que lo hemos desarrollado con Terraform, con GitLab, con CCD, entonces tenemos toda la infraestructura desarrollada como código. Las tablas, las mismas estructuras de las tablas, con Poser, etcétera. Como te decía, tenemos un after play, por así decirse, que al final hacemos que las tablas salgan de la nube, que salgan de la nube, que salgan de la nube, que salgan de la nube, que salgan de la nube, que salgan de la nube, que salgan de la nube, con un after play, por así decirse, que al final hacemos que... Tenemos un Dart template y un JSON por cada tabla. En los JSON de cada tabla tenemos cosas primordiales como el lugar donde se están installando los datos de esa tabla, la base primaria, etcétera, entonces en base a ese Dart template se genera un Dart para cada tabla. Cada vez que ejecutamos el par de las estructuras, se genera un Dart para cada tabla, etcétera. Como todos siguen este Dart template, todos tienen el mismo formato. Eso es lo principal. ¿Y cómo estructuráis los Darts de Airflow? ¿Qué son de Airflow o no? Para asegurar la reutilización, por ejemplo. La reutilización... ¿Con qué nos volvemos a la pregunta? Por ejemplo, si hace un Dart para cada cosa, o hace un Dart por cada función, lo modulariza, lo va llamando... Principalmente acá, como digo, hacemos como un Dart para... Por ejemplo, esta de Ingesta es un solo Dart que se utiliza. Y para los modelos tenemos también quizás Dart para cada modelo, pero todos siguen la misma lógica. Todos tienen los mismos pasos. Quizás, por ejemplo, cuando existe un modelo, lo que más utilizamos en Dart es ejecutar esa especie de QL para cargar el modelo. Entonces, lo más seguro es que en todos los desarrollos veas que lo que necesitas cargar es para hacer un modelo, se ejecute el mismo, como se dice, el mismo operador de Airflow para hacer este tipo de tareas. Para temas cuando llamamos a Cloud Functions, intentamos utilizar la misma forma de llamar a Cloud Functions a través del Dart. Intentamos reutilizar, pero también estamos ahí buscando mejores maneras de hacer las cosas. La experiencia en JIT... ¿Te ha tocado trabajar en JIT y te toca un conflicto de fuentes? ¿Cómo lo resuelves? ¿Conflicto de fuentes? La verdad no me ha tocado. Coméntame lo que haces en JIT, entonces. Lo que hago en JIT, más que nada, es hacer los cambios que hacían por el lado de Terraform, implementar el nuevo recurso, etc. Por el lado de los Dart, ya de los modelos, hacer cambios en los propios Dart, necesitamos nuevas formas de ejecutarlo, ya sea directamente en VML. Hago los cambios y necesitamos implementar cosas nuevas. Y para conflictos, lo más seguro es crear un rebate de lo que está, o algún reverse de lo que teníamos. Vale. ¿Experiencia en Dataflow tienes? En Dataflow sí, pero no tanto. ¿Cómo se hace un pipeline en Dataflow para procesar en tiempo real? ¿Para qué? ¿Para procesar datos en tiempo real? No, para... Ya vale. Me dijiste que de ahí conocimiento de Terraform, que hacía modificaciones también de Terraform. ¿Qué componente te ha tocado modificar en Terraform? ¿Qué componente me ha tocado? Temas de permiso de SA, agregar SA, levantar Composer, creación de tablas, levantar Parsec a través de Terraform también. ¿Esto es directo o a través de alguna herramienta de integración continua? Directo. ¿Directo? ¿En qué sentido? ¿O sea, finalizado a través del pipeline? Sí. ¿Pero ese pipeline dónde está? ¿En la universidad CD de GCP? ¿O tienen otro? De GCP. Ahí. ¿Conoce Jenkins? Lo he escuchado, pero no entiendo ni lo mismo. Si mirar ahora, no sé si me entiende, pero no he trabajado en Jenkins. Vale. Sobre el área de matemáticas, ¿te ha tocado implementar distintas aplicaciones de matemáticas en los análisis de datos? ¿De qué te refieres? No sé, ¿te ha tocado hacer algún cálculo especial? ¿Alguna fórmula? ¿Fórmula, sí? ¿Cómo rebustar? ¿Cómo rebustar? Lo típico, sumar, agitar, pero cálculo complejo dentro de la misma lógica. Ya. Y... Y eso son todas las preguntas que tenía que hacer, ¿eh? Entonces, si quieres contarme alguna otra cosa que no cubrí en las preguntas que te interese contarme técnicamente, ¿eh? No, más que nada, o sea, sé que quizás no tengo experiencia del 100% del todo, pero sabes que me gusta aprender. Si necesito meterme en algún servicio, en una revista nueva, lo más seguro es que lo pueda combinar, porque me gusta meterme en las cosas cuando no las sé, o me interesa que me las conozca. Pero más que nada, es lo que busco también. Vale. O sea, tenís una buena disposición al aprendizaje, y tú decís que aprendís rápido, también. Sí. Vale. Ya lo voy a anotar, entonces, como parte de tu habilidad. Bien, pues, José. Esas eran todas las preguntas que tenía que hacer. Y con eso yo genero un feedback, se lo envío a Matías, y ahí él va evaluando si avanza o no avanza, si cumple o no cumple, porque hay cosas que yo hago la parte técnica. Hay otras evaluaciones que yo no manejo. Que tengas mucho éxito, que te haya superado bien la postulación, y ojalá llegues a buen puerto. Vale, muchas gracias. Listo, José, cuídate. Nos vemos. Chau, chau.