¿Viniste tú, Álvaro? Gusto conocerte Álvaro Aranjuez- Bien, gracias Me llevan la atención tus apellidos ¿De dónde son? Álvaro Aranjuez- Mi abuela es irlandés y mi padre es alemán Mira, qué mezcla Álvaro Aranjuez- Sí pero yo soy más chileno que los puertos ¿Cuánto tiempo llevas acá en Chile? Álvaro Aranjuez- Toda la vida nací aquí Mi abuelo se vinieron de de esos lugares Ya, pero abuelos, digamos abuelos por un lado alemanes y por otro lado irlandeses, ¿no? Sí Ellos no se podían ver, yo creo Jajajaja Jajajaja Puro puerto matrimonial Qué simpático Cosas raras de la vida Bueno, qué simpático la tienen que haber vivido tus papás Sí Más encima, imagino en una época de conflicto político Sí, pues, también Oh, wow Qué conversaciones aquellas Sí, bueno, mis abuelos eran los abuelos de aquellos tiempos Con su personalidad fuerte Sí, pues, bueno, los alemanes y los irlandeses, ambos son de carácter fuerte Sí, sí, gracias Uno más vikingo que el otro pero igual de carácter Sí Qué interesante Bueno Álvaro, me presento, soy César Ogalde La idea es que tengamos una conversación técnica del perfil al cual estoy postulando que sería, este es Ah, Data Engineering Ya, perfecto Sí, que de repente me cambian los perfiles y me cambian las evaluaciones y yo no doy ni cuenta Entonces Entonces la idea es que tengamos una conversación técnica tratar de hacerla lo más preciso posible la respuesta para generar un feedback y la idea es conocer tus habilidades técnicas qué sé yo pero no es el objetivo profundizar sino que sea, si no sabes no hay problema Me dicen no No sé La idea es medir un poco tu experiencia más que evaluarte si tienes un 718 o no sé Perfecto Entonces partamos por las preguntas Perdón ¿Cuáles son los servicios principales de GCP que ha utilizado en proyectos y cómo los has integrado para resolver algún caso de negocio específico? Perfecto Mira Yo en GCP a nivel laboral trabajé mucho con Vertex AI ocupando un colabore Esto con esta plataforma, o sea esta herramienta la ocupamos con mi equipo para ejecutar dos diferentes pipelines de datos Necesitaba poner presento para Apple Packs También ocupamos para lo que tiene que ver con Storage y Basead ocupamos el paquete de Google Y en cuanto a Basead ocupamos mucho Bitcoin Y también algún par de productos que desarrollamos con mi equipo fueron con Big Data Vale En cuanto a los pipelines generalmente ocupamos Dataflow Ahí ocupamos más que nada Apache Beam Bueno también se ocupó PySpark pero más que nada Apache Beam por necesidades de la empresa También trabajando ahí a la par con Data Governance desarrollamos con concepto con Terraform Todo lo que tiene que ver con la infraestructura de los procesos todo lo que fuera necesario Pero eso se trabajaba más nivel o sea en conjunto con Data Governance por un tema de restricciones etc. Varias cosas que tienen que ver con cosas que no eran de mi competencia por temas de seguridad Y todo lo que tenía que ver con orquestación o automatización de procesos lo hacíamos con Airflow Y eso sería un poco los stacks tecnológicos que ocupamos ahí en en AWS Porque en estos momentos me encuentro en el ámbito Google más que te ocupamos AWS así que no entra mucho en detalle ahí porque no tiene mucho sitio Ok Respecto a Python ¿Qué librería o framework de Python utilizas para análisis de datos y cómo optimizas su código? Perfecto Para Python en cuanto a análisis de datos lo que más ocupamos es PySpark y bueno también Apache con eso es con que tomamos los datos Más que nada porque en Equifax ocupamos Big Data estamos hablando de volúmenes volúmenes muy muy altos por ejemplo un proceso pequeñito era 400GB 400GB, 500GB tenía algunos que pasaban los dos teras y también tengo otras hay otras herramientas que ocupo con Python creo que esto es más por el lado de ciencia de datos porque no está dentro de mi currículum porque no lo considero para Data Engineering pero yo llevo aproximadamente este año apoyando a mi profesor de tesis en proyectos académicos y también ocupo otras herramientas para análisis de datos como puede ser por ejemplo Pandas también ocupaba mucho Math.lib Seaborn para lo que tiene que ver con métricas y de regresiones lineales, etc. Dependería ahí pero eso es un momento que ocupo en Python Como experiencia de programación orientada a objetos ¿Me puede explicar un proyecto donde aplicó principios de programación orientada a objetos para mejorar la mantenibilidad del código? O sea, en general casi siempre yo desarrollo en orientado a objetos un tema más que nada de orden un poco de separar responsabilidades dentro del mismo proyecto y dejarlo lo más encapsulado posible cada desarrollo y un ejemplo puede ser en este caso un proyecto que hice para Equifax que fue una malla familiar en este caso lo que yo tuve que generar fue como un grafo gigante de el el estado crediticio de cada persona en Chile y esto tenía que quedar de forma tenía que mostrarse en forma de grafo un grafo bidireccional entonces ahí claro yo ocupé programación orientada a objetos más que nada porque había que optimizar el código porque el código que había antes era mezclaba entre BigQuery y Python para desarrollar esta maquina ya existía un desarrollo pero era carísimo y era muy lento entonces yo lo pasé todo esto a full Python y luego de eso el resultado se pasaba a un Bigtable para ocupar el grafo entonces claro ahí ocupé por un lado programación orientada a objetos para crear el grafo como tal pero también creé varias herramientas para por ejemplo búsqueda por Antube etc. todo eso a través de programación orientada a objetos y con eso se optimizó ese desarrollo que antes por ejemplo demoraba por decir algo aproximadamente era como un día tres cuartos casi dos días y costaba aproximadamente dos mil dólares de ejecución que era muy muy pesado había que hacer una malla familiar de todo Chile y lo otro era como con 10 generaciones hacia atrás porque ese requerimiento no tengo idea pero eran 10 generaciones entonces nosotros tomamos datos de servicio impuesto interno que era una tabla de todo Chile eran más o menos 400 GB y resultaba en un Bigtable que es más reducido en nuestro dato y quedaba por ejemplo de como 3 TB el resultado a lo mejor estaban buscando las familias más ricas de Chile claro, claro exactamente entonces era salió a tu ¿alcanzaste a ver tu árbol? ¿tuviste la curiosidad? tuve la curiosidad pero no me fueron temas de de no querer meterme en problemas no quise ahondar en en aquello porque realmente uno puede encontrar cosas raras de las familias entonces mejor que buena política si no quieres saber mejor que no averigüe pero por porque el que busca encuentra la habilidad laboral preferí no y bueno, reduje el tiempo a más o menos 6 horas, 8 horas de ejecución y el pricing o el costo de la ejecución bajó de más o menos de 1.800 a casi 2.000 dólares a 600, 700 dólares impresionante ¿y cómo redujiste ahí? ¿fue con esta cosa la lista de objetos? principalmente claro, o sea se ocupaba los nodos o la forma en que se generaban los nodos era una forma muy extraña me costó mucho entenderlo porque como que concatenaban las relaciones entre personas y esa concatenación era muy costosa claro, generar un string con los claves mezcladas, ¿no? claro, exactamente por ejemplo estaba hablando de algo que tal vez N cubo no, eso otra vez como A N log N más o menos no, tu hiciste referencias de grafo orientado a objetos así recursivo y de hecho te achicó hasta el código me imagino que no, y el código claro también era una brutanía porque no estaba hecho con imperativo, o sea así como sí, sí modo tabular tipo COBOL exactamente, entonces era muy complejo entender ese desarrollo y más que nada ese desarrollo se hizo porque había que hacerlo me acuerdo que me comentaron que se hizo sobre la marcha porque era un producto que era necesario sacarlo y lo sacaron lo antes posible sin importar que fuera lento y costoso, pero había que sacarlo muy rápido, entonces no había buenas prácticas de desarrollo simplemente se sacó para no estar un problema que había que hacerlo ya y ya hubo más tiempo para para su buen desarrollo, buen levantamiento de requerimientos, etc los SLA que se necesitaban ya se armó algo más robusto respecto a la expedición SQL ¿cómo optimizaría una consulta que tarda demasiado tiempo en ejecutarse? o ¿cuáles serían los pasos que tu ocupas para analizar eso? perfecto generalmente lo que hago yo es revisar si es posible por ejemplo desanidar consultas una de las primeras cosas que veo ¿qué otras cosas podría revisar? si es que se puede por ejemplo ocupar procedimiento almacenado también un poco de ligare dependencia crear por ejemplo pinta puede ser que en su momento habría que analizar el proyecto pero generalmente creo que una de las cosas que más más incide es la anidación de repente uno por sacar una consulta propia empieza a anidar anidar ¿cómo se llama esto? queries ahora si por ejemplo estamos hablando de Big Data estamos hablando de un poco más a nivel también si es que por ejemplo se puede hacer una indexación veríase que se puede analizar una indexación de las tablas eso a veces también reduce el costo, aumenta obviamente el tamaño de la tabla pero pero se puede reducir el tiempo con indexación vale respecto a modelamiento, integración y procesamiento descríbe un proyecto donde diseñaste algún modelo de datos eficiente y con qué fuentes lo integraste perfecto aquí yo creo que tal vez mi mi el proyecto emblema es uno que hice aquí en el banco no es con GCP pero obviamente igual es SQL realmente o sea es un motor de datos que de todas formas ocupa SQL así que sigo con más o menos con esa línea y fue crear un modelo de datos para para el área de seguros del banco ahí tenían todo muy desordenado es un mundo súper complejo porque tiene mil y un seguros entonces lo que hice yo más que nada fue como aplicar lean qué es lo que se necesita, qué es lo que no se necesita ya no solamente con lo importante y nada, o sea hacer una buena toma de requerimiento ordenar un poco el tema y ahí me di cuenta por ejemplo que los seguros en el banco por lo menos se separan en dos grandes áreas uno que era los credit related que les llame yo que son todos aquellos seguros que se generan una vez uno toma un crédito porque hay que tener un seguro cuando uno toma un crédito y los open market que en este caso serían aquellos que no necesitan crédito por ejemplo un seguro de gravamen o segurería etc vale, ya que no están asociados claro, sí y qué herramientas ocupé ahí en ese momento a mí me gusta mucho trabajar con Workbench Workbench SQL siempre lo traje con eso o de repente ocupo diagram.net para mostrar lo que quiero así a nivel para el cliente, explicarle más o menos cómo se va a orquestar el modelo y eso, eso más que nada porque claro Workbench generalmente es un poco más técnico porque te muestra las ideas entonces con diagram.net yo tomo el Workbench y lo bajo un poquito de nivel para que se vea solamente como hay las relaciones y eso mostrarsele al cliente y explicarle cómo se va a consumir o explotar ese modelo vale con respecto a BigQuery, ¿cómo manejas la optimización de costo al ejecutar consultas BigQuery? lo que yo hago, lo que yo hice mucho en en Network Tax fue tratar de no generar tablas muy horizontales o sea, porque BigQuery practica mucho cuando la tabla es horizontal entonces lo que yo hice fue generar por ejemplo una clave una clave de búsqueda o clave primaria y un un campo donde tuvieran todos los atributos de cada registro y esto fuera un array o sea, por ejemplo, tengo root1 y adentro todos los atributos de esa, o sea, perdón adentro de un array de registros aquí en el campo y después busco el arrays, por ejemplo eso es lo que más ocupe la herramienta más la que más ocupe y otra cosa es que yo hacía mucho para bajar para bajar los costos de sobre todo para el desarrollo era hacer una una query donde yo sacaba como un datamock así como un conjunto de datos de prueba para por ejemplo mi ETL porque cuando uno hace una query en BigQuery incluso si uno pone limit o se le hace un limitador igual te lee toda la tabla o sea, entonces una de las cosas que yo hacía mucho que no tiene que ver con la parte como de sino que tiene que ver más con el desarrollo era eso, siempre tener una tabla de mock de prueba para mi ETL etcétera, para no hacer consultas tan grandes entonces eso es una de las cosas que también ocupaba Vale, ¿ocupaste Apache Airflow? ¿me contaste? Sí, ocupé Airflow ¿Cómo estructura los DACs en Apache Airflow para asegurar la escalabilidad y la reutilización? Realmente nunca hice un análisis así como específico porque Airflow generalmente yo lo ocupo a nivel banco lo ocupo en banco no hay Big Data, entonces no entro mucho en análisis simplemente creo lo que tengo que crear según necesidad en Equitax si era necesario sin embargo en Equitax se trabajaba mucho con como con una metodología de hazlo nomás aunque los costos y los tiempos fueran muy grandes era más importante siempre tener el desarrollo al principio y luego ver si era posible optimizar entonces en cuanto a escalabilidad realmente teníamos mucha bastante espacio para poder desarrollar ahí claro, si, entonces nunca fue necesario hacer un análisis así muy exhaustivo de escalabilidad en escalabilidad siempre se han pasado por dos procesos un proceso de tener los productos y luego un proceso ya de optimización, por ejemplo en el caso de la mayoría familiar que te he explicado pero si, nunca fue necesario así como revisar tanto a nivel escalabilidad, porque generalmente lo que hacemos mucho en Equitax como los conjuntos eran muy grandes, era desde ya generar la mayor optimización posible de la ETL como tal y dejar el flow más que nada como un orquestador lo que va más bien el orquestador y en la ETL no ocuparse de la optimización de los costos ok, en JIT te ha tocado trabajar con JIT? si, o sea de hecho casi todo lo hago en JIT como gestionar el conflicto en JIT cuando está en un flujo de trabajo colaborativo nada, cada uno trabaja con su rama ya lo importante siempre es el tema del flujo de trabajo, separar separar en master, en develop pua, etc y importante siempre que el push que vaya a ser algún colaborador esté actualizado con la rama a la que está apuntando, o sea por ejemplo generalmente uno trabaja en DEP hacer un Git pull origin a lo que hay para revisar que no haya conflicto vale y siempre trabajar sobre develop y después de develop pasar por pua y una vez que pua está ok, pasar a a master disculpa que cambie tema porque me queda poco tiempo cuando tocó ver Terraform que hiciste con Terraform? Terraform generalmente como digo lo trabajamos mucho con Data Governance si bien lo que yo hacía era bajar un poco la carga laboral a Data Governance en Equifax porque ahí es donde traje Terraform, no lo traje mucho aquí en el banco me tocó hacer un par de infraestructuras pero siempre de la mano con Data Governance porque es la que te dice oye si puedes ocupar esto o no puedes ocupar esto porque ellos tenían el acceso ya a nivel de infraestructura y a nivel plataforma de habilitar la infraestructura del de la GCP al fin y al cabo todos mis desarrollos tenían que pasar tenían que tener Terraform por un tema de buenas prácticas pero al fin y al cabo era Data Governance quien toma la última decisión si es que el file del Terraform era plausible o no de ocupar vale en Jenkins no sé si tuviste experiencia en Jenkins si como se implementa un pipeline en Jenkins para los despliegues mira te voy a explicar un proyecto que hice con Jenkins que fue un motor de Ukiah este motor de Ukiah trabajaba con BigTable de BigQuery, o sea perdón de GCP, perdón y estos motores de Ukiah eran para volúmenes muy grandes y lo que uno hacía era como generar llaves a través de un YAML entonces la persona tenía que trabajar sobre una rama en específico y agregarle un tag ese tag tenía una nomenclatura que era necesaria para varias reglas de negocio del desarrollo entonces una persona subía un tag más el YAML que era como la llave de Ukiah para generar la Ukiah y creo que ese fue como mi proyecto emblema con Jenkins porque Jenkins generalmente lo uso para cosas más simples por ejemplo si quiero automatizar ejecución, etc. hago Jenkins pero generalmente más Airflow para la automatización y orquestaciones en Equipax Jenkins se ocupaba más para para entregas continuas pero para procesos que tenían que ver más con motores motores de Ukiah, motores de recomendaciones, etc. eso ya más de la mano con Data Science Vale, respecto a matemáticas, ¿qué aplicaciones de matemáticas has utilizado en los análisis o modelos de datos? Ya, yo creo que ahí miría más por el lado de Data Science, ahí con mi profesor o sea, en matemáticas ocupamos mucho todo lo que tiene que ver con Red Neuronales por ejemplo no sé si pactan datos y hay que llenar se pueden ocupar por ejemplo regresiones lineales regresiones polinómicas logísticas, si es que los datos son son categorizables no, un largo etc. o sea, Mate ocupó mucho a nivel Data Science a nivel Data Engineering no ocupó mucho no ocupó mucho porque siempre esa tarea, la que te acabo de escribir la Ben, los Data Science o los Data Analytics de la organización o sea, a nivel Banco y a nivel Equipax generalmente todo lo que tiene que ver con saneamiento de datos a través de, por ejemplo, no sé de, o digo, regresiones etc. lo pedía ese equipo, a nosotros nos pedían más que nada como disponibilizar la data a nivel Cloud para poder trabajar eh pero yo podría decir que a nivel como matemática Data Engineering, tal vez mi proyecto emblema es lo que te digo, la mayoría familiar donde se ocupó hay búsqueda por altura como lo más cercano pero eso perfecto, y que dos últimas preguntas con esto vamos cerrando sobre integración continua y continuo delivery ¿cuál estrategia aplicas para asegurar la calidad en un pipeline? eh o sea, más que nada lo que hago yo siempre es tratar de eh, hacer el desarrollo lo más rapido posible, lo más escalable posible cumplir los los requerimientos que se piden, las reglas de negocio eh y en cuanto a a lo que tiene que ver con entrega continua, lo que siempre busco es que el es que el proceso como tal eh el kit, que se yo si es que voy a ocupar Jenkins que el Jenkins y todo esté lo más eh, pulido posible vale eh, pero en general eso que el repositorio de kit esté bien ordenado que no se sostene que los Jenkins que vaya a ocupar estén bien diseñados eh más que nada eso vale lo que pasa es que a nivel siempre lo que esperaba más es que el producto salga y luego uno pasa por una etapa ya más de optimización vale ya yo terminé con las preguntas espero no te haya estresado tanto la idea no es estresar sino que como ir midiendo las habilidades técnicas así que yo con esta información genero un feedback ese feedback se lo voy a entregar a Matías Matías ahí avanzará y te comentará cuáles son los siguientes pasos, vale así que Álvaro, un gusto conocerte que tengáis harto éxito y que te vaya súper bien el proceso vale, gracias nos vemos, chau chau Subtítulos realizados por la comunidad de Amara.org